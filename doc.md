# Basic evolutionary algorithm

Initialization -> loop(Selection -> Variation -> Evaluation) -> Termination

Variation = Recombination -> Mutation (on the offsprings)

# Representation

- direct representation
- indirect representation (can simplify the design of good variation operators)
  - cycle representation
  - permutation representation
  - adjacent representation
- penalty method: the search space is enlarged to D′, whose elements can be represented, and a penalty term is added to the objective function, penalizing violation of the constraints (soft or hard constraint).

The basic variation operators will be applied to the representation.

Self-adaptivity or hyperparameter search?

# Initialization

- Monte Carlo sampling

# Selection

- fitness-proportional selection
  - sigma scaling selection 
  - ranking selection
- Competition-based selection 
  - top-k selection 
  - k-tournament selection 
  - round-robin tournament selection (not recommended)

- ranking (selection pressure)
  - linear decay
  - quadratic decay
  - exponential decay

Selection pressure parameter s is often fixed, but it can also decay according to a schedule. When exponential decay ranking is chosen, the candidate solutions with the best objective value are strongly favored as s decreases. This can benefit the convergence and termination. 

# Variation
The trade-off between exploration and exploitation is partly determined by suitable variation  operators that introduce randomness and simultaneously amplify promising features of  the candidate solutions that were selected.

## Crossover/Recombination variation
Recombination of two individuals with a good objective value should lead to offspring that are close (in some distance) to the two parents. Additionally, some randomness in the generation of offspring is advised.  Recombination is typically applied with high probability (say 100%)

Recombination is more complicated and technical: See pages 65–74 in Eiben and Smith.

## Mutation variation
mutation should function like (local) random search and the probability of applying it to a candidate solution should be not too high (say 5%).

- Insert mutation
- Swap mutation
- Inversion mutation
- Scramble mutation

See slides lecture 2 for more mutation operators.

# Evaluation/Elimination

There are no fundamental differences between selection and elimination. Hence, the  foregoing selection strategies can all be used for elimination. In this case, sample without replacement.

- age-based elimination: only the new candidate solutions generated during the variation phase are retained
- the candidate solutions with the worst fitness values are eliminated
- elitism: in which the top-k individuals in the combined seed population and offspring are retained.
- (λ + μ)-elimination: The seed population and the offspring are merged (resulting in a population of size λ + μ) and then the top-λ candidate solutions are retained.
- (λ, μ)-elimination: The seed population is discarded and the top-λ candidate solutions from the offspring are retained. This requires μ > λ, typically μ/λ ≈ 5.

# Termination

# Local search

The goal of a local search operator is to locally improve one candidate solution, typically by a computationally cheap heuristic. This can guide the search process towards more interesting areas of the search space.

Initialization -> Local search -> loop(Selection -> Local search -> Variation -> Evaluation) -> Termination

In the initialization phase, the population can be enriched by 
- adding solutions generated by other heuristic methods 
- adding solutions generated by previous runs with different parameters
- applying a local search heuristic to randomly generated individuals 

Always keep a portion of the population randomly initialized to ensure sufficient diversity of the candidate solutions.

After applying recombination to generate offspring, local search
optimization can be applied to
- all offspring, 
- a random subset of the offspring, 
- the worst individuals, or 
- to the parents.

k-opt local search is a good choice for the TSP.

## Practical consideration 
Local search operators typically have tunable parameters (e.g., k in k-opt). It is recommended to make them part of the representation of a candidate solution and use self-adaptivity to learn them.

Loss of diversity: The goal of a local search operator is to push candidate
solutions towards more promising areas of the search space. This causes
more exploitation of local information. It can happen that the local search
is exploiting too much and ignoring large areas of the search space.
To combat this problem, there are several options:
- you can modify the variation operators to introduce more randomness;
- you should limit the range of allowed self-adaptable parameters;
- you could modify the elimination mechanism to explicitly promote the diversity in the population, e.g., in (λ + μ)-selection whenever an individual is selected, the most similar individual from the remaining population is eliminated.

# Promoting diversity

- Crowding
- Island model
- Fitness sharing

# Multi-objective optimization

- Fixed tradeoff or scalarization
- island model
- Pareto front approximation
